{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae6376-2b14-4ce0-86d6-54f313cf04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d164a30-e815-4884-8f0f-e830664159e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Utility: patient-level normalization\n",
    "# ------------------------------------------------------------\n",
    "def patient_scale(bags):\n",
    "    \"\"\"\n",
    "    bags: dict patient_id -> tensor [N_i, d]\n",
    "    Returns normalized version.\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    for pid, X in bags.items():\n",
    "        mean = X.mean(dim=0, keepdim=True)\n",
    "        std  = X.std(dim=0, keepdim=True)\n",
    "        scaled[pid] = (X - mean) / (std + 1e-6)\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. MIL attention model with DROPOUT\n",
    "# ------------------------------------------------------------\n",
    "class AttentionMIL(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.a = nn.Linear(in_dim, hidden_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.b = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.tanh(self.a(x))\n",
    "        h = self.drop(h)\n",
    "        scores = self.b(h)               # [N, 1]\n",
    "        weights = torch.softmax(scores, dim=0)\n",
    "        pooled = torch.sum(weights * x, dim=0, keepdim=True)\n",
    "        return pooled, weights.squeeze()\n",
    "\n",
    "\n",
    "class MILModel(nn.Module):\n",
    "    def __init__(self, emb_dim, att_dim=128, cls_dim=64, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.attention = AttentionMIL(emb_dim, att_dim, dropout=dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(emb_dim, cls_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(cls_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, bag):\n",
    "        pooled, att = self.attention(bag)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits, att\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Multi-sample Dataset + Add Noise\n",
    "# ------------------------------------------------------------\n",
    "class MultiSampleRandomBags(Dataset):\n",
    "    def __init__(self, bags, labels, n_samples=1000, K=5, noise_std=0.02):\n",
    "        self.bags = [torch.as_tensor(b, dtype=torch.float32) for b in bags]\n",
    "        self.labels_list = torch.as_tensor(labels, dtype=torch.float32)\n",
    "        self.n_samples = n_samples\n",
    "        self.K = K\n",
    "        self.noise_std = noise_std\n",
    "        self.num_patients = len(bags)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_patients * self.K\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx = idx // self.K\n",
    "        bag = self.bags[patient_idx]\n",
    "\n",
    "        # sample cells\n",
    "        N = bag.shape[0]\n",
    "        if N > self.n_samples:\n",
    "            idxs = torch.randint(0, N, (self.n_samples,))\n",
    "            bag = bag[idxs]\n",
    "        else:\n",
    "            bag = bag.clone()\n",
    "\n",
    "        # add Gaussian noise (on CPU)\n",
    "        noise = self.noise_std * torch.randn_like(bag)\n",
    "        bag = bag + noise\n",
    "\n",
    "        label = self.labels_list[patient_idx]\n",
    "\n",
    "        return bag, label\n",
    "\n",
    "def collate_bags(batch):\n",
    "    bags, labels = zip(*batch)\n",
    "    bags = [b.float() for b in bags]                  # ensures tensor\n",
    "    labels = torch.stack([torch.tensor(l) for l in labels]).float()\n",
    "    return bags, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Train function with class-balanced BCE\n",
    "# ------------------------------------------------------------\n",
    "def train_mil(model, dataloader, epochs=20, lr=1e-3, device=\"cuda\"):\n",
    "\n",
    "    labels_all = dataloader.dataset.labels_list\n",
    "    pos = (labels_all == 1).sum()\n",
    "    neg = (labels_all == 0).sum()\n",
    "    pos_weight = (neg / (pos + 1e-6)).float().to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "\n",
    "        for bags, labels in dataloader:\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0\n",
    "\n",
    "            for bag, label in zip(bags, labels):\n",
    "                bag = bag.to(device)\n",
    "                logits, _ = model(bag)\n",
    "                loss = criterion(logits.squeeze(), label)\n",
    "                batch_loss += loss\n",
    "\n",
    "            batch_loss /= len(bags)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += batch_loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} LOSS {loss_sum/len(dataloader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Predict on full bag\n",
    "# ------------------------------------------------------------\n",
    "def predict_patient(model, bag, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, att = model(bag.to(device))\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "    return prob, att.cpu().numpy()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. LOO Cross-Validation\n",
    "# ------------------------------------------------------------\n",
    "def loo_cv(bags, labels_dict, n_samples=1000, epochs=20, K=5, lr=1e-3,\n",
    "           att_dim=128, cls_dim=64, device=\"cuda\"):\n",
    "\n",
    "    results = []\n",
    "    patient_ids = list(bags.keys())\n",
    "\n",
    "    for left_out in patient_ids:\n",
    "        print(f\"\\n=== LOO: Leaving out {left_out} ===\")\n",
    "\n",
    "        train_ids = [pid for pid in patient_ids if pid != left_out]\n",
    "\n",
    "        train_bags = [bags[pid] for pid in train_ids]\n",
    "        train_labels = torch.tensor([labels_dict[pid] for pid in train_ids])\n",
    "\n",
    "        ds = MultiSampleRandomBags(train_bags, train_labels,\n",
    "                                   n_samples=n_samples,\n",
    "                                   K=K,\n",
    "                                   noise_std=0.02)\n",
    "\n",
    "        loader = DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_bags)\n",
    "\n",
    "        emb_dim = next(iter(bags.values())).shape[1]\n",
    "        model = MILModel(emb_dim, att_dim=att_dim, cls_dim=cls_dim)\n",
    "        model = model.to(device)   # <-- FIX HERE\n",
    "        model = train_mil(model, loader, epochs=epochs, lr=lr, device=device)\n",
    "\n",
    "        # evaluate\n",
    "        prob, att = predict_patient(model, bags[left_out], device=device)\n",
    "        results.append({\"sample\": left_out,\n",
    "                        \"prob\": prob,\n",
    "                        \"true\": labels_dict[left_out],\n",
    "                        \"attention\": att})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. GROUP K-FOLD Cross-Validation\n",
    "# ------------------------------------------------------------\n",
    "def group_kfold_cv(bags, labels_dict, groups, n_splits=5,\n",
    "                   n_samples=1000, epochs=20, K=5,\n",
    "                   lr=1e-3, att_dim=128, cls_dim=64,\n",
    "                   device=\"cuda\"):\n",
    "\n",
    "    \"\"\"\n",
    "    groups: array of group identifiers, length = #patients\n",
    "    \"\"\"\n",
    "\n",
    "    patient_ids = list(bags.keys())\n",
    "    X_dummy = np.zeros(len(patient_ids))    # sklearn requires X, doesn't matter\n",
    "    y_dummy = np.zeros(len(patient_ids))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for train_idx, test_idx in gkf.split(X_dummy, y_dummy, groups=groups):\n",
    "        train_ids = [patient_ids[i] for i in train_idx]\n",
    "        test_ids  = [patient_ids[i] for i in test_idx]\n",
    "\n",
    "        print(\"\\n=== Group Fold ===\")\n",
    "        print(\"Train:\", train_ids)\n",
    "        print(\"Test :\", test_ids)\n",
    "\n",
    "        train_bags = [bags[p] for p in train_ids]\n",
    "        train_labels = torch.tensor([labels_dict[p] for p in train_ids])\n",
    "\n",
    "        ds = MultiSampleRandomBags(train_bags, train_labels,\n",
    "                                   n_samples=n_samples,\n",
    "                                   K=K,\n",
    "                                   noise_std=0.02)\n",
    "\n",
    "        loader = DataLoader(ds, batch_size=4, shuffle=True, collate_fn=collate_bags)\n",
    "\n",
    "        emb_dim = next(iter(bags.values())).shape[1]\n",
    "        model = MILModel(emb_dim, att_dim=att_dim, cls_dim=cls_dim)\n",
    "        model = train_mil(model, loader, epochs=epochs, lr=lr, device=device)\n",
    "\n",
    "        # test fold\n",
    "        for pid in test_ids:\n",
    "            prob, att = predict_patient(model, bags[pid], device=device)\n",
    "            results.append({\"sample\": pid,\n",
    "                            \"prob\": prob,\n",
    "                            \"true\": labels_dict[pid],\n",
    "                            \"attention\": att})\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780566b-15f5-4cf3-aae4-4f124d2d271c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923d6e7-8c81-437b-b513-6c2ec1d585c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_small_clusters(\n",
    "    df: pd.DataFrame,\n",
    "    cluster_col: str,\n",
    "    min_count: int = 1000,\n",
    "    new_label: str = \"small_clusters\",\n",
    "    output_col: str = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups small clusters in a DataFrame column into a single label.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing cluster labels.\n",
    "        cluster_col (str): Name of the column containing cluster labels (e.g., 'leiden').\n",
    "        min_count (int): Minimum number of entries a cluster must have to avoid grouping.\n",
    "        new_label (str): Label to assign to small clusters.\n",
    "        output_col (str or None): Name of the new column to store grouped labels. \n",
    "                                  If None, defaults to '{cluster_col}_grouped'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the DataFrame with a new column containing grouped cluster labels.\n",
    "    \"\"\"\n",
    "    if cluster_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{cluster_col}' not found in DataFrame.\")\n",
    "\n",
    "    output_col = output_col or f\"{cluster_col}_grouped\"\n",
    "    cluster_counts = df[cluster_col].value_counts()\n",
    "    small_clusters = cluster_counts[cluster_counts < min_count].index\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df[output_col] = df[cluster_col].astype(str)\n",
    "    new_df.loc[df[cluster_col].isin(small_clusters), output_col] = new_label\n",
    "\n",
    "    return new_df[output_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb15164-e143-4583-9068-1ec2a0d4b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6857e9-f5b4-48fd-8789-3de3d28f1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_info = pd.read_csv('../../../Broad_SpatialFoundation/VisiumHD-LUAD/clinical-info/full_clinical.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7614e86-27f6-4e6c-b24d-e00e12b28779",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pl.Path('../../../Broad_SpatialFoundation/VisiumHD-LUAD-processed/')\n",
    "sample_list = np.setdiff1d([f.stem for f in base_dir.iterdir()],['full_cohort','LIB-064888st1'])\n",
    "sample_list = sample_list.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa84cb-5e99-406b-a70d-4295abf1b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6898c11-1e0a-4391-bbb8-d62fa9fc97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_obs = pd.read_parquet('../../../Broad_SpatialFoundation/notebooks/nsclc_adata_obs.parquet')\n",
    "\n",
    "adata_obs['leiden_joint'] = group_small_clusters(\n",
    "    adata_obs[['leiden']],\n",
    "    cluster_col='leiden',\n",
    "    min_count= 1000,\n",
    "    new_label= \"Other\",\n",
    "    output_col = None\n",
    ")\n",
    "\n",
    "malignant_niches = ['0','1','2','4','5','7','10','12','15','16',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08046f0b-3f52-4ac7-9644-215b179c2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = {}\n",
    "for sample in sample_list:\n",
    "    embeddings_df[sample] = pd.read_parquet(base_dir / f'{sample}/embeddings/NicheFinder.parquet')\n",
    "    embeddings_df[sample].columns = embeddings_df[sample].columns.astype(str) \n",
    "    embeddings_df[sample] = embeddings_df[sample][[f'{i}' for i in range(10)]]\n",
    "    embeddings_df[sample].index =  embeddings_df[sample].index + '::' + sample\n",
    "    embeddings_df[sample] = embeddings_df[sample].loc[embeddings_df[sample].index.intersection(adata_obs.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47dac7-6f07-4889-889c-ae7ae0d10cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adata_obs[['sample_id','leiden_joint']]\n",
    "leiden = {}\n",
    "for sample_id in df.sample_id.unique():\n",
    "    leiden[sample_id] = df.loc[df.sample_id==sample_id][['leiden_joint']].astype(str)\n",
    "\n",
    "sub_embeddings = {}\n",
    "for sample in embeddings_df:\n",
    "    sub_embeddings[sample] = embeddings_df[sample].loc[leiden[sample].index]\n",
    "    sub_embeddings[sample] = sub_embeddings[sample].loc[leiden[sample]['leiden_joint'].isin(malignant_niches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa9489-ab69-4236-88a9-c6de934eb30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sub_embeddings.items():\n",
    "    print(k)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a60257-e477-4cd5-897e-2798832d504e",
   "metadata": {},
   "source": [
    "# Only with malignant niches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a22b1-7ba0-4789-9981-b98f3289e5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = {}\n",
    "for sample_id, df in sub_embeddings.items():\n",
    "    bags[sample_id] = torch.tensor(df.values.astype(float), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# enforce alignment (important!)\n",
    "sample_ids = list(bags.keys())\n",
    "bag_list = [bags[s] for s in sample_ids]\n",
    "label_list = torch.tensor([labels_dict[s] for s in sample_ids], dtype=torch.float32)\n",
    "\n",
    "print(\"Example bag shape:\", bag_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc9a14-31a6-4215-9e56-78d48da59aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_scaled = patient_scale(bags)   # IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d2d44-ea40-4152-8853-4ecf8264bc51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "N_RUNS = 15\n",
    "all_run_results = []\n",
    "all_metrics = []\n",
    "\n",
    "# NEW: store attention per sample per run\n",
    "attention_store = {s: [] for s in bags_scaled.keys()}\n",
    "\n",
    "tgt = 'pTNM T red'\n",
    "targets = clinical_info[['Library',tgt]].set_index('Library')\n",
    "targets = pd.get_dummies(targets).astype(int)\n",
    "targets = targets.iloc[:,0].to_frame()\n",
    "targets.columns = [\"target\"]\n",
    "labels_dict = targets[\"target\"].to_dict()\n",
    "\n",
    "for run in range(1, N_RUNS + 1):\n",
    "    print(f\"\\n\\n##############################\")\n",
    "    print(f\"###      RUN {run} / {N_RUNS}      ###\")\n",
    "    print(f\"##############################\\n\")\n",
    "\n",
    "    # run LOO\n",
    "    results = loo_cv(\n",
    "        bags_scaled,\n",
    "        labels_dict,\n",
    "        n_samples=1000,\n",
    "        epochs=15,\n",
    "        K=10,\n",
    "        lr=1e-3,\n",
    "        att_dim=128,\n",
    "        cls_dim=64,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # extract predictions\n",
    "    df = pd.DataFrame([\n",
    "        {\"sample\": r[\"sample\"], \"true\": r[\"true\"], \"prob\": r[\"prob\"]}\n",
    "        for r in results\n",
    "    ])\n",
    "    df[\"run\"] = run\n",
    "    all_run_results.append(df)\n",
    "\n",
    "    # NEW — store attention maps\n",
    "    for r in results:\n",
    "        sample = r[\"sample\"]\n",
    "        att = r[\"attention\"]  # vector of length (#cells in patient)\n",
    "        attention_store[sample].append(att)\n",
    "\n",
    "    # compute metrics for this run\n",
    "    auc = roc_auc_score(df[\"true\"], df[\"prob\"])\n",
    "    acc = balanced_accuracy_score(df[\"true\"], df[\"prob\"] > 0.5)\n",
    "    f1  = f1_score(df[\"true\"], df[\"prob\"] > 0.5)\n",
    "\n",
    "    print(df)\n",
    "    print(f\"Run {run} — AUC: {auc:.3f}, BAC: {acc:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    all_metrics.append({\"run\": run, \"auc\": auc, \"acc\": acc, \"f1\": f1})\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Combine per-run metrics\n",
    "# ------------------------------\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n\\n==========================================\")\n",
    "print(\"###  PER-RUN METRICS  ###\")\n",
    "print(\"==========================================\")\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  AGGREGATE PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "print(f\"AUC: {metrics_df['auc'].mean():.3f} ± {metrics_df['auc'].std():.3f}\")\n",
    "print(f\"BAC: {metrics_df['acc'].mean():.3f} ± {metrics_df['acc'].std():.3f}\")\n",
    "print(f\"F1:  {metrics_df['f1'].mean():.3f} ± {metrics_df['f1'].std():.3f}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Averaged ensemble predictions\n",
    "# ------------------------------\n",
    "all_df = pd.concat(all_run_results)\n",
    "\n",
    "ensemble_df = (\n",
    "    all_df.groupby(\"sample\")\n",
    "          .agg(true=(\"true\", \"mean\"),   # truth is identical across runs\n",
    "               prob=(\"prob\", \"mean\"))\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "ensemble_auc = roc_auc_score(ensemble_df[\"true\"], ensemble_df[\"prob\"])\n",
    "ensemble_acc = balanced_accuracy_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "ensemble_f1  = f1_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  ENSEMBLE (AVERAGED-PROB) PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "print(ensemble_df)\n",
    "\n",
    "print(f\"Ensemble AUC: {ensemble_auc:.3f}\")\n",
    "print(f\"Ensemble BAC: {ensemble_acc:.3f}\")\n",
    "print(f\"Ensemble F1:  {ensemble_f1:.3f}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# NEW: Compute averaged attention maps\n",
    "# -------------------------------\n",
    "avg_attention = {}\n",
    "\n",
    "for sample, att_list in attention_store.items():\n",
    "    # each entry is a vector of size (#cells in sample)\n",
    "    A = np.stack(att_list, axis=0)     # shape = [num_runs, num_cells]\n",
    "    avg_attention[sample] = A.mean(axis=0)\n",
    "\n",
    "# Now avg_attention[sample] is the consensus attention per cell\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  STORED & AVERAGED ATTENTION MAPS ###\")\n",
    "print(\"==========================================\")\n",
    "for sample in avg_attention:\n",
    "    print(sample, \"attention shape:\", avg_attention[sample].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ff1e4-7bf1-44e2-886e-2433c92c4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_auc = roc_auc_score(ensemble_df[\"true\"], ensemble_df[\"prob\"])\n",
    "ensemble_acc = balanced_accuracy_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "ensemble_f1  = f1_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  ENSEMBLE (AVERAGED-PROB) PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "print(ensemble_df)\n",
    "\n",
    "print(f\"Ensemble AUC: {ensemble_auc:.3f}\")\n",
    "print(f\"Ensemble BAC: {ensemble_acc:.3f}\")\n",
    "print(f\"Ensemble F1:  {ensemble_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6219f-d729-46d9-9da1-95ad981a3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.to_csv('ensemble_pred_pTNMred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e460f45-e9cd-4f65-934f-273604daef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spl in avg_attention:\n",
    "    pd.DataFrame(avg_attention[spl], index=sub_embeddings[spl].index, columns=['Avg attention']).to_parquet(f'{spl}_avg_attention.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535384e-7407-41bb-8af3-d5dc94901c8d",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d8247-bbc2-4472-9895-82b4da4889fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['svg.fonttype'] = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4859d3-f657-447c-88c6-e7e95e4d76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df = pd.read_csv('ensemble_pred_pTNMred.csv', index_col=0)\n",
    "\n",
    "avg_attention = {}\n",
    "for spl in sample_list:\n",
    "    avg_attention[spl]= pd.read_parquet(f'{spl}_avg_attention_pTNM_T_red.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224d847-605a-4b7f-a4fd-badb5c3c740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# ================================\n",
    "# Set Nature Genetics–like styling\n",
    "# ================================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.labelsize\": 13,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"figure.dpi\": 120,\n",
    "})\n",
    "\n",
    "# Extract values\n",
    "y_true = ensemble_df[\"true\"].values\n",
    "y_prob = ensemble_df[\"prob\"].values\n",
    "y_pred = (ensemble_df[\"prob\"] > 0.5).astype(int)\n",
    "\n",
    "# =======================================\n",
    "# PLOT 1 — ROC CURVE (Nature Genetics look)\n",
    "# =======================================\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "ax.plot(fpr, tpr, color=\"#2c7fb8\", lw=3, label=f\"AUC = {roc_auc:.3f}\")\n",
    "ax.plot([0, 1], [0, 1], color=\"black\", lw=1.2, linestyle=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curve\")\n",
    "ax.legend(loc=\"lower right\", frameon=False)\n",
    "\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "fig.savefig('../../../SpatialFusion/results/figures_Fig6/roc_auc_abmil_pTNM_T_red.svg')\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# PLOT 2 — CONFUSION MATRIX (publication quality)\n",
    "# ==========================================\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(3,2))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "    cbar=False, square=True, ax=ax,\n",
    "    annot_kws={\"size\": 16}\n",
    ")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "\n",
    "fig.savefig('../../../SpatialFusion/results/figures_Fig6/confusion_abmil_pTNM_T_red.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95382d-f64b-4eb2-b5e0-d9f1a1b12483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_cells = []\n",
    "\n",
    "for sample in avg_attention:\n",
    "    att = avg_attention[sample].values.ravel()                        # vector (#cells,)\n",
    "    idx = sub_embeddings[sample].index             # same order as bag                    # match your adata_obs index\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"attention\": att,\n",
    "        \"sample\": sample,\n",
    "        \"cell_id\": idx,\n",
    "        \"global_id\": idx\n",
    "    })\n",
    "    df = df.set_index(\"global_id\")\n",
    "    # bring in metadata (cluster, subtype, coordinates,…)\n",
    "    df = pd.concat([df, adata_obs.loc[df.index]],axis=1)\n",
    "\n",
    "    all_cells.append(df)\n",
    "\n",
    "all_cells_df = pd.concat(all_cells)\n",
    "print(\"Combined shape:\", all_cells_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886dda18-3df0-47a9-9b10-dd47a12cc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_df[\"att_norm\"] = (\n",
    "    all_cells_df.groupby(\"sample\")[\"attention\"]\n",
    "    .transform(lambda x: x / x.sum())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f13cf4-c5e5-446b-bd8e-d5eb41d5b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTILE = 0.95   # top 5% attention cells\n",
    "\n",
    "top_cells = (\n",
    "    all_cells_df.groupby(\"sample\")\n",
    "    .apply(lambda df: df[df.attention >= df.attention.quantile(QUANTILE)])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Top cells shape:\", top_cells.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48c15f-773b-4abe-a1fa-27b583c28185",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "\n",
    "for sample in avg_attention:\n",
    "    df_all = all_cells_df[all_cells_df[\"sample\"] == sample]\n",
    "    \n",
    "    baseline[sample] = {\n",
    "        \"leiden\": df_all[\"leiden_joint\"].value_counts(normalize=True),\n",
    "        \"subtype\": df_all[\"refined_cellsubtypes\"].value_counts(normalize=True),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c613f4-1607-49aa-8290-a1cdd686a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact, chi2_contingency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "results = []\n",
    "\n",
    "TOP_Q = 0.95  # e.g., top 5% attention\n",
    "\n",
    "for sample in avg_attention:\n",
    "    df_all = all_cells_df[all_cells_df[\"sample\"] == sample]\n",
    "\n",
    "    # Identify top-attention cells\n",
    "    cutoff = df_all[\"attention\"].quantile(TOP_Q)\n",
    "    df_top = df_all[df_all[\"attention\"] >= cutoff]\n",
    "\n",
    "    # Baseline & observed counts\n",
    "    total_cells = len(df_all)\n",
    "    top_cells = len(df_top)\n",
    "\n",
    "    # Process Leiden clusters\n",
    "    for cluster in df_all[\"leiden_joint\"].unique():\n",
    "\n",
    "        k_obs = (df_top[\"leiden_joint\"] == cluster).sum()            # in top\n",
    "        k_exp = (df_all[\"leiden_joint\"] == cluster).sum()            # baseline\n",
    "        k_not_obs = top_cells - k_obs                                # not cluster in top\n",
    "        k_not_exp = total_cells - k_exp                              # not cluster in sample\n",
    "\n",
    "        # 2×2 table\n",
    "        table = np.array([\n",
    "            [k_obs,    k_not_obs],\n",
    "            [k_exp-k_obs, k_not_exp-k_not_obs]\n",
    "        ])\n",
    "\n",
    "        # Fisher exact test\n",
    "        try:\n",
    "            odds, p = fisher_exact(table)\n",
    "        except:\n",
    "            odds, p = np.nan, 1.0\n",
    "\n",
    "        # Log2 fold-change enrichment\n",
    "        frac_top = k_obs / top_cells if top_cells > 0 else 0\n",
    "        frac_all = k_exp / total_cells\n",
    "        log2fc = np.log2((frac_top + 1e-6) / (frac_all + 1e-6))\n",
    "\n",
    "        # Store\n",
    "        results.append({\n",
    "            \"sample\": sample,\n",
    "            \"cluster\": cluster,\n",
    "            \"k_obs\": k_obs,\n",
    "            \"k_exp\": k_exp,\n",
    "            \"top_frac\": frac_top,\n",
    "            \"baseline_frac\": frac_all,\n",
    "            \"log2FC\": log2fc,\n",
    "            \"p_fisher\": p,\n",
    "            \"odds_ratio\": odds\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df[\"q_fisher\"] = multipletests(results_df[\"p_fisher\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73258969-a035-4345-8979-1bb39ee6025c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.sort_values('odds_ratio',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59115e5-f3d3-451c-b0a6-fc80f8378b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_enrichment(results_df, centroids_df, cluster_palette, savefig=None):\n",
    "    \"\"\"\n",
    "    results_df: per-sample results\n",
    "    centroids_df: aggregated cluster centers\n",
    "    cluster_palette: dict {cluster: (r,g,b)} from palettes[\"leiden_joint\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # Safely compute -log10(q)\n",
    "    # Replace q=0 with the minimum positive value\n",
    "    # ------------------------------\n",
    "    q = results_df[\"q_fisher\"].replace(0, results_df[\"q_fisher\"][results_df[\"q_fisher\"] > 0].min())\n",
    "    results_df = results_df.assign(logp = -np.log10(q))\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create figure\n",
    "    # ------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "\n",
    "    # Consistent ordering and palette\n",
    "    unique_clusters = sorted(results_df[\"cluster\"].unique())\n",
    "    palette_used = {c: cluster_palette[str(c)] for c in unique_clusters}\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1. Individual sample points\n",
    "    # ------------------------------\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=results_df,\n",
    "        x=\"log2FC\",\n",
    "        y=\"logp\",\n",
    "        hue=\"cluster\",\n",
    "        palette=palette_used,\n",
    "        s=10,\n",
    "        alpha=0.7,\n",
    "        linewidth=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Centroids (bigger, diamond marker)\n",
    "    # ------------------------------\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=centroids_df,\n",
    "        x=\"log2FC_median\",\n",
    "        y=\"logp_median\",\n",
    "        hue=\"cluster\",\n",
    "        palette=palette_used,\n",
    "        s=30,\n",
    "        marker=\"D\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.3,\n",
    "        alpha=0.9,\n",
    "        legend=False   # avoid duplicate legends\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Reference lines\n",
    "    # ------------------------------\n",
    "    ax.axvline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "    ax.axhline(1, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Labels & title\n",
    "    # ------------------------------\n",
    "    ax.set_xlabel(\"log2 Fold-Change (top-attention vs background)\", fontsize=14)\n",
    "    ax.set_ylabel(\"-log10(FDR)\", fontsize=14)\n",
    "    ax.set_title(\"Niche Enrichment in High-Attention Cells\", fontsize=16)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Legend outside, no frame\n",
    "    # ------------------------------\n",
    "    legend = ax.legend(\n",
    "        title=\"Niche\",\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        loc=\"upper left\",\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    # Increase dot size in legend\n",
    "    # Use modern handle list: legend.legend_handles\n",
    "    for h in legend.legend_handles:\n",
    "        try:\n",
    "            h.set_sizes([70])\n",
    "        except Exception:\n",
    "            pass  # some handles may not support set_sizes\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        fig.savefig(savefig)\n",
    "    plt.show()\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e8d92-025c-4ac0-81b9-f6596ce8c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../../Broad_SpatialFoundation/notebooks/palettes_NSCLC_Novartis.json\", \"r\") as f:\n",
    "    palettes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9debd-27d4-43ce-938d-3a806c50a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cluster_enrichment(\n",
    "    results_df=results_df,\n",
    "    centroids_df=centroids,\n",
    "    cluster_palette=palettes[\"leiden_joint\"],\n",
    "    savefig='../../../SpatialFusion/results/figures_Fig6/volcano_attention_leiden.svg',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada731cc-5e95-487a-8aae-b22ab2dc7849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===============================\n",
    "# Global Nature Genetics styling\n",
    "# ===============================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 14,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.linewidth\": 1.2,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"figure.dpi\": 200,\n",
    "})\n",
    "\n",
    "# A nicer red colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "NG_REDS = LinearSegmentedColormap.from_list(\n",
    "    \"NG_Reds\",\n",
    "    [\"#fee5d9\", \"#fcae91\", \"#fb6a4a\", \"#de2d26\", \"#a50f15\"]\n",
    ")\n",
    "\n",
    "def plot_attention_map_NG(sample, savefig=None):\n",
    "    df = all_cells_df[all_cells_df[\"sample\"] == sample]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(3.1, 3))\n",
    "\n",
    "    # Scatter: Nature style = slightly larger points, calm alpha\n",
    "    sc = ax.scatter(\n",
    "        df[\"X\"], df[\"Y\"],\n",
    "        c=df[\"attention\"],\n",
    "        cmap=NG_REDS,\n",
    "        s=4,\n",
    "        alpha=0.85,\n",
    "        edgecolors=\"none\",\n",
    "        rasterized=True,\n",
    "    )\n",
    "\n",
    "    # Title\n",
    "    ax.set_title(f\"Attention map — {sample}\", pad=12)\n",
    "\n",
    "    # Reverse Y for Visium\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    # Remove spines\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    # Remove axis ticks and labels for a clean NG look\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    # Add a slim colorbar\n",
    "    cbar = fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Attention\", rotation=270, labelpad=15)\n",
    "    cbar.outline.set_linewidth(0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig is not None:\n",
    "        fig.savefig(savefig, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Plot all samples\n",
    "# ===============================\n",
    "for spl in all_cells_df['sample'].unique():\n",
    "    print(spl)\n",
    "    plot_attention_map_NG(spl, savefig=f'../../../SpatialFusion/results/figures_Fig6/{spl}_attn_map.svg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773a681-d2a3-420f-99b7-25eb0b0a844a",
   "metadata": {},
   "source": [
    "# Dissociated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d09062-f22d-4cda-a2fe-d748c7e8e1f6",
   "metadata": {},
   "source": [
    "Here we try to do ABMIL with the gene expression to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdef96-7ad7-49dd-b7d3-3e188b85a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad190fc-c6a9-4ad0-b67a-f90bcfc3fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pl.Path('../../../Broad_SpatialFoundation/VisiumHD-LUAD-processed/')\n",
    "sample_list = np.setdiff1d([f.stem for f in base_dir.iterdir()],['full_cohort','LIB-064888st1'])\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787e4c9-51e0-4d01-a33f-299180c27eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = {}\n",
    "for sample in tqdm(sample_list):\n",
    "    adata = sc.read_h5ad(base_dir / sample / 'adata.h5ad')\n",
    "    adata.obs_names = adata.obs_names + '::' + sample \n",
    "    adata.obs['sample_id'] = sample\n",
    "    \n",
    "    common_idx = adata.obs_names.intersection(embeddings_df[sample].index)\n",
    "    adata = adata[common_idx].copy()\n",
    "    adata = adata[adata.obs.celltypes != 'Noise'].copy()\n",
    "    adatas[sample] = adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6403e-7522-4b56-96e2-7f92b1f84569",
   "metadata": {},
   "outputs": [],
   "source": [
    "gex_embeddings = {}\n",
    "for sample in tqdm(sample_list):\n",
    "    sc.pp.normalize_total(adatas[sample], target_sum=10000)\n",
    "    sc.pp.log1p(adatas[sample])\n",
    "    sc.tl.pca(adatas[sample])\n",
    "    gex_embeddings[sample] = adatas[sample].obsm['X_pca'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e66ca9-c376-4ec2-ab3c-fdb036310e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = {}\n",
    "\n",
    "for sample_id, df in gex_embeddings.items():\n",
    "    bags[sample_id] = torch.tensor(df.astype(float), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# enforce alignment (important!)\n",
    "sample_ids = list(bags.keys())\n",
    "bag_list = [bags[s] for s in sample_ids]\n",
    "label_list = torch.tensor([labels_dict[s] for s in sample_ids], dtype=torch.float32)\n",
    "\n",
    "print(\"Example bag shape:\", bag_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc895c10-9172-4212-86fa-350e72791c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_scaled = patient_scale(bags)   # IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d2495-cbd9-4653-98f1-09c5d9832b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, balanced_accuracy_score\n",
    "\n",
    "N_RUNS = 15\n",
    "all_run_results = []\n",
    "all_metrics = []\n",
    "\n",
    "# NEW: store attention per sample per run\n",
    "attention_store = {s: [] for s in bags_scaled.keys()}\n",
    "\n",
    "tgt = 'pTNM T red'\n",
    "targets = clinical_info[['Library',tgt]].set_index('Library')\n",
    "targets = pd.get_dummies(targets).astype(int)\n",
    "targets = targets.iloc[:,0].to_frame()\n",
    "targets.columns = [\"target\"]\n",
    "labels_dict = targets[\"target\"].to_dict()\n",
    "\n",
    "for run in range(1, N_RUNS + 1):\n",
    "    print(f\"\\n\\n##############################\")\n",
    "    print(f\"###      RUN {run} / {N_RUNS}      ###\")\n",
    "    print(f\"##############################\\n\")\n",
    "\n",
    "    # run LOO\n",
    "    results = loo_cv(\n",
    "        bags_scaled,\n",
    "        labels_dict,\n",
    "        n_samples=1000,\n",
    "        epochs=15,\n",
    "        K=10,\n",
    "        lr=1e-3,\n",
    "        att_dim=128,\n",
    "        cls_dim=64,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # extract predictions\n",
    "    df = pd.DataFrame([\n",
    "        {\"sample\": r[\"sample\"], \"true\": r[\"true\"], \"prob\": r[\"prob\"]}\n",
    "        for r in results\n",
    "    ])\n",
    "    df[\"run\"] = run\n",
    "    all_run_results.append(df)\n",
    "\n",
    "    # NEW — store attention maps\n",
    "    for r in results:\n",
    "        sample = r[\"sample\"]\n",
    "        att = r[\"attention\"]  # vector of length (#cells in patient)\n",
    "        attention_store[sample].append(att)\n",
    "\n",
    "    # compute metrics for this run\n",
    "    auc = roc_auc_score(df[\"true\"], df[\"prob\"])\n",
    "    acc = balanced_accuracy_score(df[\"true\"], df[\"prob\"] > 0.5)\n",
    "    f1  = f1_score(df[\"true\"], df[\"prob\"] > 0.5)\n",
    "\n",
    "    print(df)\n",
    "    print(f\"Run {run} — AUC: {auc:.3f}, BAC: {acc:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    all_metrics.append({\"run\": run, \"auc\": auc, \"acc\": acc, \"f1\": f1})\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Combine per-run metrics\n",
    "# ------------------------------\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "print(\"\\n\\n==========================================\")\n",
    "print(\"###  PER-RUN METRICS  ###\")\n",
    "print(\"==========================================\")\n",
    "print(metrics_df)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  AGGREGATE PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "print(f\"AUC: {metrics_df['auc'].mean():.3f} ± {metrics_df['auc'].std():.3f}\")\n",
    "print(f\"BAC: {metrics_df['acc'].mean():.3f} ± {metrics_df['acc'].std():.3f}\")\n",
    "print(f\"F1:  {metrics_df['f1'].mean():.3f} ± {metrics_df['f1'].std():.3f}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Averaged ensemble predictions\n",
    "# ------------------------------\n",
    "all_df = pd.concat(all_run_results)\n",
    "\n",
    "ensemble_df = (\n",
    "    all_df.groupby(\"sample\")\n",
    "          .agg(true=(\"true\", \"mean\"),   # truth is identical across runs\n",
    "               prob=(\"prob\", \"mean\"))\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "ensemble_auc = roc_auc_score(ensemble_df[\"true\"], ensemble_df[\"prob\"])\n",
    "ensemble_acc = balanced_accuracy_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "ensemble_f1  = f1_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  ENSEMBLE (AVERAGED-PROB) PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "print(ensemble_df)\n",
    "\n",
    "print(f\"Ensemble AUC: {ensemble_auc:.3f}\")\n",
    "print(f\"Ensemble BAC: {ensemble_acc:.3f}\")\n",
    "print(f\"Ensemble F1:  {ensemble_f1:.3f}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# NEW: Compute averaged attention maps\n",
    "# -------------------------------\n",
    "avg_attention = {}\n",
    "\n",
    "for sample, att_list in attention_store.items():\n",
    "    # each entry is a vector of size (#cells in sample)\n",
    "    A = np.stack(att_list, axis=0)     # shape = [num_runs, num_cells]\n",
    "    avg_attention[sample] = A.mean(axis=0)\n",
    "\n",
    "# Now avg_attention[sample] is the consensus attention per cell\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  STORED & AVERAGED ATTENTION MAPS ###\")\n",
    "print(\"==========================================\")\n",
    "for sample in avg_attention:\n",
    "    print(sample, \"attention shape:\", avg_attention[sample].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088d393-44c7-4a3c-ba3f-d8c906af5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_auc = roc_auc_score(ensemble_df[\"true\"], ensemble_df[\"prob\"])\n",
    "ensemble_acc = balanced_accuracy_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "ensemble_f1  = f1_score(ensemble_df[\"true\"], ensemble_df[\"prob\"] > 0.5)\n",
    "\n",
    "print(\"\\n==========================================\")\n",
    "print(\"###  ENSEMBLE (AVERAGED-PROB) PERFORMANCE  ###\")\n",
    "print(\"==========================================\")\n",
    "print(ensemble_df)\n",
    "\n",
    "print(f\"Ensemble AUC: {ensemble_auc:.3f}\")\n",
    "print(f\"Ensemble BAC: {ensemble_acc:.3f}\")\n",
    "print(f\"Ensemble F1:  {ensemble_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40bf7a-5e42-4478-b18e-0d0207778275",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_df.to_csv('ensemble_pred_GEX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0e46f-5970-4e0e-a26d-66b943e79755",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spl in avg_attention:\n",
    "    pd.DataFrame(avg_attention[spl], index=adatas[spl].obs_names, columns=['Avg attention']).to_parquet(f'{spl}_avg_attention_GEX.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098005ad-0e83-4f16-979c-060d5d7efdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaa4a2-72e0-4750-bfa7-8a9eaed2ea0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
