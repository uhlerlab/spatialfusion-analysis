{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bec1b-ad3f-4856-85ad-94468ac55d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415036f-6802-419c-81f0-01fed0fd112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spatialfusion.embed.embed import load_gcn, gcn_embeddings_from_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b1f4c-73d7-416a-a9a9-1e8cf966e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0d06b-16cc-46ad-8b5b-8ec6395b08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9c06d-cdd9-4f52-ac86-56a4a7ebc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd0aeb-f4f6-4fae-b6aa-5c54f162d1dd",
   "metadata": {},
   "source": [
    "# Full GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f821e2-aa23-4b52-b4e3-ae3b3c3659d4",
   "metadata": {},
   "source": [
    "Load configuration that was used for the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedbeef-da95-4ce4-ad02-5dd4969fea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"../../results/checkpoint_dir_gcn/checkpoint_dir_gcn/gcn_20251022-170459_5a4f4f64/config_5a4f4f64.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bfa83-1461-4d6b-8537-3e0cbef834a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = pl.Path(cfg.dataset.datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f8f76-5b94-4a33-aa53-d28a0b80422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_by_sample = {}\n",
    "for sample in tqdm(cfg.dataset.test_samples):\n",
    "    adata_by_sample[sample] = sc.read_h5ad(datapath / sample / 'adata.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af857059-15db-4833-b39e-7068878d7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_joint_df = pd.read_parquet('../../results/embeddings_ae/full-AE-output-model-3085dad0/z_joint_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdd24c-f89c-423c-96b8-2672c60f0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model_dir = pl.Path('../../spatialfusion/data/checkpoint_dir_gcn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca174d9-4213-4ceb-9ceb-13ecb72215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you concatenated AE joints across samples (index = cell ids from all samples)\n",
    "# and you have each sample's adata in a dict.\n",
    "# Keys must match the sample names you want in the output metadata.\n",
    "\n",
    "gcn_model = load_gcn(gcn_model_dir / \"spatialfusion-full-gcn.pt\", in_dim=z_joint_df.shape[1], device=\"cuda\")\n",
    "\n",
    "gcn_emb_df = gcn_embeddings_from_joint(\n",
    "    gcn_model=gcn_model,\n",
    "    z_joint=z_joint_df,                # one big joint embedding over all cells\n",
    "    adata_by_sample=adata_by_sample,   # per-sample AnnData objects\n",
    "    base_path=\".\",                     # path for metadata lookups/saves\n",
    "    device=\"cuda\",\n",
    "    spatial_key=\"spatial_he\",             # or \"spatial_px\"\n",
    "    celltype_key=\"major_celltype\",\n",
    "    k=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00325d25-5ea5-4062-b047-64c7c5bf7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_emb_df.to_parquet('../../results/embeddings_gcn/spatialfusion-full-gcn/SpatialFusion.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7572161-db76-4927-9fe4-0836cfecff4b",
   "metadata": {},
   "source": [
    "# GCN without pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9e05b-9c57-4ce1-a1c8-e259ec328b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"../../results/checkpoint_dir_gcn/checkpoint_dir_gcn/gcn_20251022-170720_4e2cecfe/config_4e2cecfe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a67a6-401a-4e02-a920-7be409a84e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = pl.Path(cfg.dataset.datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ecd8a-297e-4d37-a111-db1194cb6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_by_sample = {}\n",
    "for sample in tqdm(cfg.dataset.test_samples):\n",
    "    adata_by_sample[sample] = sc.read_h5ad(datapath / sample / 'adata.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991e6df-c86a-46ea-94b6-51c0e2dbdc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_joint_df = pd.read_parquet('../../results/embeddings_ae/full-AE-output-model-3085dad0/z_joint_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edecd891-219b-4363-940d-bf9ebcce9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model_dir = pl.Path('../../results/checkpoint_dir_gcn/checkpoint_dir_gcn/gcn_20251022-170720_4e2cecfe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f75889-33f6-46bb-a4e7-6d5681b02132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you concatenated AE joints across samples (index = cell ids from all samples)\n",
    "# and you have each sample's adata in a dict.\n",
    "# Keys must match the sample names you want in the output metadata.\n",
    "\n",
    "gcn_model = load_gcn(gcn_model_dir / \"model.pt\", in_dim=z_joint_df.shape[1], device=\"cuda\")\n",
    "\n",
    "gcn_emb_nopathway_df = gcn_embeddings_from_joint(\n",
    "    gcn_model=gcn_model,\n",
    "    z_joint=z_joint_df,                # one big joint embedding over all cells\n",
    "    adata_by_sample=adata_by_sample,   # per-sample AnnData objects\n",
    "    base_path=\".\",                     # path for metadata lookups/saves\n",
    "    device=\"cuda\",\n",
    "    spatial_key=\"spatial_he\",             # or \"spatial_px\"\n",
    "    celltype_key=\"major_celltype\",\n",
    "    k=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e98a8-aeb1-46aa-b4f4-18e1e8db220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_emb_nopathway_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ad20e-3f0a-4532-903d-892e2d9ca670",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_emb_nopathway_df.to_parquet('../../results/embeddings_gcn/spatialfusion-nopathway-gcn/SpatialFusion.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90cf6d-f8b1-4833-ad4c-76a914bf565a",
   "metadata": {},
   "source": [
    "# Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7cdd01-e662-4954-9229-480aa393a7ee",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d8263-7706-46db-bd57-f15c7980b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(emb_df, n_components=5):\n",
    "    drop_cols = ['sample_id',]\n",
    "    # Add CNiche and TNiche if they exist in the DataFrame\n",
    "    drop_cols += [col for col in [ 'cell_id', 'X_coord', 'Y_coord', 'CNiche', 'TNiche', 'cellsubtype','celltype', 'cellsubtypes','celltypes', \n",
    "                                  'CCL','CXCL','PD-L1','CD86','EGF','CEACAM','VEGF'] if col in emb_df.columns]\n",
    "\n",
    "    features = emb_df.drop(columns=drop_cols)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(features)\n",
    "    pc_df = pd.DataFrame(pcs, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "    return pd.concat([emb_df.reset_index(drop=True), pc_df], axis=1)\n",
    "\n",
    "\n",
    "def standardize_pathways(df: pd.DataFrame, method: str = \"robust_z\", eps: float = 1e-6, tol: float = 1e-3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Column-wise standardization of pathway scores.\n",
    "    - 'robust_z': (x - median) / IQR  (safer to outliers / skew)\n",
    "    - 'z':        (x - mean) / std\n",
    "    Columns where all values are nearly zero (|x| < tol) are set to 0.\n",
    "    NaNs and infs are replaced by 0.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Detect \"almost-zero\" columns\n",
    "    all_near_zero = (df.abs().max(axis=0) < tol)\n",
    "\n",
    "    if method == \"z\":\n",
    "        mu = df.mean(axis=0)\n",
    "        sigma = df.std(axis=0).replace(0, np.nan)\n",
    "        out = (df - mu) / (sigma + eps)\n",
    "    else:  # robust z-score\n",
    "        med = df.median(axis=0)\n",
    "        q1 = df.quantile(0.25, axis=0)\n",
    "        q3 = df.quantile(0.75, axis=0)\n",
    "        iqr = (q3 - q1).replace(0, np.nan)\n",
    "        out = (df - med) / (iqr + eps)\n",
    "\n",
    "    # Force \"uninformative\" pathways to 0s after scaling\n",
    "    out.loc[:, all_near_zero] = 0.0\n",
    "\n",
    "    # Clean up numerical edge cases\n",
    "    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(np.float32)\n",
    "    return out\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def plot_pca_grid(pc_df, color_by_list, max_samples=None, group_by='patient_id', cmap=\"vlag\", ncols=2, savefig=None):\n",
    "    \"\"\"\n",
    "    Plot PCA scatterplots for multiple features on a grid (e.g., 2 columns).\n",
    "    Styled for publication-quality figures (e.g., Nature Genetics).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pc_df : pd.DataFrame\n",
    "        Must contain columns 'PC1', 'PC2', and each variable in color_by_list.\n",
    "    color_by_list : list of str\n",
    "        Columns to color points by (categorical or continuous).\n",
    "    max_samples : int, optional\n",
    "        Max number of samples per group (defined by group_by).\n",
    "    group_by : str\n",
    "        Column to balance subsampling across.\n",
    "    cmap : str\n",
    "        Colormap for continuous variables (default 'vlag').\n",
    "    \"\"\"\n",
    "\n",
    "    df = pc_df.copy()\n",
    "\n",
    "    # --- Subsample evenly across groups ---\n",
    "    if max_samples is not None and group_by in df.columns:\n",
    "        df = (\n",
    "            df.groupby(group_by, group_keys=False)\n",
    "              .apply(lambda g: g.sample(min(len(g), max_samples), random_state=42),\n",
    "                     include_groups=False)\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # --- Figure setup ---\n",
    "    n = len(color_by_list)\n",
    "    nrows = math.ceil(n / ncols)\n",
    "\n",
    "    sns.set_context(\"talk\", font_scale=1.2)\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5.5 * ncols, 5 * nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i, color_by in enumerate(color_by_list):\n",
    "        ax = axes[i]\n",
    "        if color_by not in df.columns:\n",
    "            raise ValueError(f\"Column '{color_by}' not found in dataframe.\")\n",
    "\n",
    "        # --- Continuous vs categorical ---\n",
    "        if np.issubdtype(df[color_by].dtype, np.number):\n",
    "            norm = plt.Normalize(df[color_by].min(), df[color_by].max())\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "\n",
    "            sc = ax.scatter(\n",
    "                df[\"PC1\"], df[\"PC2\"],\n",
    "                c=df[color_by], cmap=cmap, norm=norm,\n",
    "                s=15, alpha=0.8, edgecolor='none'\n",
    "            )\n",
    "\n",
    "            cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.ax.tick_params(labelsize=25)\n",
    "            cbar.set_label(color_by, fontsize=25)\n",
    "\n",
    "        else:\n",
    "            sns.scatterplot(\n",
    "                data=df,\n",
    "                x=\"PC1\", y=\"PC2\",\n",
    "                hue=color_by,\n",
    "                s=15, alpha=0.8,\n",
    "                ax=ax, linewidth=0, legend=False\n",
    "            )\n",
    "\n",
    "        # --- Titles and axes ---\n",
    "        ax.set_title(f\"{color_by}\", fontsize=30,)\n",
    "        ax.set_xlabel(\"PC1\", fontsize=25)\n",
    "        ax.set_ylabel(\"PC2\", fontsize=25)\n",
    "\n",
    "        # --- Remove ticks but keep axis labels ---\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # --- Remove spines (Nature style) ---\n",
    "        for spine in [\"top\", \"right\", \"left\", \"bottom\"]:\n",
    "            ax.spines[spine].set_visible(False)\n",
    "\n",
    "        # --- Clean layout ---\n",
    "        ax.grid(False)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=9)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savefig is not None:\n",
    "        fig.savefig(savefig, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfe027e-8ca0-45f5-8741-72245849d8f3",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c503-3c38-4c79-aefe-7a6d709a11fa",
   "metadata": {},
   "source": [
    "Download the two embeddings + the pathway per sample to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c367ef-f49b-4949-baab-c8865856ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_emb_df = pd.read_parquet('../../results/embeddings_gcn/spatialfusion-full-gcn/SpatialFusion.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6858d40-6df8-415f-bcb3-f2a9fc91d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_emb_nopathway_df = pd.read_parquet('../../results/embeddings_gcn/spatialfusion-nopathway-gcn/SpatialFusion.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1f19e-e230-49cd-9733-7322714ecdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.load(\"/../../checkpoint_dir_gcn/checkpoint_dir_gcn/gcn_20251022-170720_4e2cecfe/config_4e2cecfe.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161231f-052f-4bd7-947c-12f690b72a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = pl.Path(cfg.dataset.datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfde34-794a-40a4-81fb-76fd1eb47c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pw = []\n",
    "for patient in cfg['dataset']['test_samples']:\n",
    "    pw = pd.read_parquet(datapath / patient / 'pathway_activation.parquet')\n",
    "    pw = standardize_pathways(pw)\n",
    "    all_pw.append(pw)\n",
    "all_pw = pd.concat(all_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e97f93-5594-480f-b057-6e28eaa6362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = run_pca(gcn_emb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e247ba0-bc24-4f64-b0c3-ef580b9e8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_nopath_df = run_pca(gcn_emb_nopathway_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b02de-3bfc-420a-a228-c2c708e77b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat([pc_df.set_index('cell_id')[[f'PC{i}' for i in range(1,6)]],all_pw],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a242a8-bfc5-46af-bc0c-b3d5a10202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "figdir = '../../results/figures_Fig1/'\n",
    "os.makedirs(figdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930bd66-568d-4900-9ac2-43f0c989d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_grid(plot_df, color_by_list=all_pw.columns, max_samples=100000, group_by='sample_id', cmap=\"vlag\", ncols=5,\n",
    "              savefig=pl.Path(figdir) / 'pathway_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846fbc2-b371-42fb-b80a-6ee187b4dcfa",
   "metadata": {},
   "source": [
    "## Quantify organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a756907-deb7-4e2e-8515-3525274f00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "import pynndescent\n",
    "\n",
    "# ----------------------------\n",
    "# kNN via pynndescent (approx)\n",
    "# ----------------------------\n",
    "def _pynndescent_knn(X_f32, k=15):\n",
    "    print(f\"[kNN] Building pynndescent index with k={k} ...\")\n",
    "    t0 = time.time()\n",
    "    index = pynndescent.NNDescent(\n",
    "        X_f32, n_neighbors=k+1, metric=\"euclidean\", random_state=0, n_jobs=-1\n",
    "    )\n",
    "    idx, dists = index.neighbor_graph  # distances (not squared)\n",
    "    print(f\"[kNN] Done. Took {time.time()-t0:.2f} sec\")\n",
    "    # Drop self-neighbor at [:,0]\n",
    "    return idx[:, 1:], dists[:, 1:]\n",
    "\n",
    "# ----------------------------\n",
    "# Moran's I (streamed)\n",
    "# ----------------------------\n",
    "def morans_I_stream(y_values: np.ndarray, knn_idx: np.ndarray, knn_d2: np.ndarray,\n",
    "                    inverse_distance=True, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute Moran's I from neighbor lists only (no sparse W).\n",
    "    knn_idx: (n, k) neighbor indices\n",
    "    knn_d2:  (n, k) squared distances\n",
    "    \"\"\"\n",
    "    n, k = knn_idx.shape\n",
    "    z = y_values - y_values.mean()\n",
    "    z2_sum = float((z ** 2).sum())\n",
    "\n",
    "    if inverse_distance:\n",
    "        w = 1.0 / (np.sqrt(knn_d2) + eps)\n",
    "    else:\n",
    "        w = np.ones_like(knn_d2, dtype=np.float32)\n",
    "\n",
    "    z_i = z[:, None]\n",
    "    z_neighbors = z[knn_idx]\n",
    "    num_directed = float(np.sum(w * z_i * z_neighbors))\n",
    "    S0 = float(np.sum(w))\n",
    "    I = (n / S0) * (num_directed / z2_sum)\n",
    "    return I, S0\n",
    "\n",
    "# ----------------------------\n",
    "# CV R^2 on pre-standardized X (reusing splits)\n",
    "# ----------------------------\n",
    "def cv_r2_linear_from_X(X_std: np.ndarray, y: np.ndarray, splits, alpha=1.0):\n",
    "    \"\"\"\n",
    "    X_std: standardized features (n, d)\n",
    "    y:     target (n,)\n",
    "    splits: iterable of (train_idx, test_idx)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i, (tr, te) in enumerate(splits, 1):\n",
    "        fold_t0 = time.time()\n",
    "        model = Ridge(alpha=alpha, solver=\"sag\", random_state=0,\n",
    "                      max_iter=10_000, tol=1e-3)\n",
    "        model.fit(X_std[tr], y[tr])\n",
    "        yhat = model.predict(X_std[te])\n",
    "        ss_res = float(np.sum((y[te] - yhat) ** 2))\n",
    "        ss_tot = float(np.sum((y[te] - np.mean(y[te])) ** 2))\n",
    "        r2 = 1.0 - ss_res / (ss_tot + 1e-12)\n",
    "        scores.append(r2)\n",
    "        print(f\"[CV] Fold {i} R^2={r2:.4f} (took {time.time()-fold_t0:.2f} sec)\")\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "# ----------------------------\n",
    "# Shared subsample (same cells for both embeddings & all Y columns)\n",
    "# ----------------------------\n",
    "def _shared_subsample_indices_multi(embeddings_df: pd.DataFrame,\n",
    "                                    embeddings_df_old: pd.DataFrame,\n",
    "                                    Y_df: pd.DataFrame,\n",
    "                                    subsample_n: int,\n",
    "                                    seed: int = 0):\n",
    "    # Only keep rows where ALL Y columns are non-null to keep neighbors valid for every variable\n",
    "    Y_valid_idx = Y_df.dropna(how=\"any\").index\n",
    "    common_all = embeddings_df.index.intersection(embeddings_df_old.index).intersection(Y_valid_idx)\n",
    "    n_common = len(common_all)\n",
    "    if n_common == 0:\n",
    "        raise ValueError(\"No overlapping cell names among embeddings_df, embeddings_df_old, and non-null rows of Y_df.\")\n",
    "    take_n = min(subsample_n, n_common)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    take_idx = rng.choice(n_common, size=take_n, replace=False)\n",
    "    sub_index = common_all.take(take_idx)\n",
    "    return sub_index\n",
    "\n",
    "# ----------------------------\n",
    "# End-to-end with shared subsample for MULTIPLE variables\n",
    "# ----------------------------\n",
    "def compare_embeddings_subsampled_samecells_multi(embeddings_df: pd.DataFrame,\n",
    "                                                  embeddings_df_old: pd.DataFrame,\n",
    "                                                  Y_df: pd.DataFrame,\n",
    "                                                  k: int = 15,\n",
    "                                                  subsample_n: int = 100_000,\n",
    "                                                  inverse_distance: bool = True,\n",
    "                                                  seed: int = 0,\n",
    "                                                  n_splits: int = 5,\n",
    "                                                  alpha: float = 1.0):\n",
    "    \"\"\"\n",
    "    Subsamples the SAME ~subsample_n cells from the triple intersection of\n",
    "    {embeddings_df, embeddings_df_old, Y_df (non-null across all columns)} and runs, for each Y column:\n",
    "      - pynndescent kNN (once per embedding)\n",
    "      - Moran's I\n",
    "      - n-fold Ridge CV R^2 (reusing the same splits across variables)\n",
    "    Returns a nested dict and a tidy DataFrame for convenience.\n",
    "    \"\"\"\n",
    "    # ---------- select shared subsample ----------\n",
    "    print(f\"[Setup] Selecting shared subsample (up to {subsample_n:,} cells, seed={seed}) ...\")\n",
    "    t0 = time.time()\n",
    "    sub_index = _shared_subsample_indices_multi(embeddings_df, embeddings_df_old, Y_df, subsample_n, seed)\n",
    "    print(f\"[Setup] Chosen {len(sub_index):,} shared cells in {time.time()-t0:.2f} sec.\")\n",
    "\n",
    "    # Slice Y once\n",
    "    Y_sub = Y_df.loc[sub_index].astype(np.float32)\n",
    "    y_cols = list(Y_sub.columns)\n",
    "    print(f\"[Setup] {len(y_cols)} variables: {y_cols}\")\n",
    "\n",
    "    # Prepare CV splits ONCE and reuse across variables and embeddings\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    # We need the length n for splits; base it on the subsample size\n",
    "    n = len(sub_index)\n",
    "    dummy_indices = np.arange(n)\n",
    "    splits = list(kf.split(dummy_indices))\n",
    "\n",
    "    results = {\"new\": {}, \"old\": {}}\n",
    "    tidy_rows = []\n",
    "\n",
    "    # ---------- per-embedding pipeline (neighbors once, reuse for all y cols) ----------\n",
    "    for tag, df in [(\"new\", embeddings_df), (\"old\", embeddings_df_old)]:\n",
    "        print(f\"\\n=== Processing embedding: {tag} ===\")\n",
    "        t_total = time.time()\n",
    "\n",
    "        # Extract X for the shared subsample\n",
    "        print(\"[Data] Extracting X for shared subsample ...\")\n",
    "        t1 = time.time()\n",
    "        X = df.loc[sub_index].to_numpy(dtype=np.float32)\n",
    "        print(f\"[Data] Done in {time.time()-t1:.2f} sec. Using {X.shape[0]:,} cells and {X.shape[1]} dims.\")\n",
    "\n",
    "        # Standardize features once\n",
    "        print(\"[Preproc] Standardizing features ...\")\n",
    "        t1 = time.time()\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        X_std = scaler.fit_transform(X).astype(np.float32)\n",
    "        print(f\"[Preproc] Done. Took {time.time()-t1:.2f} sec\")\n",
    "\n",
    "        # Neighbors once per embedding\n",
    "        idx, dists = _pynndescent_knn(X_std, k=k)\n",
    "        d2 = (dists.astype(np.float32)) ** 2  # squared distances for Moran's I\n",
    "\n",
    "        # For each variable column\n",
    "        for c_idx, col in enumerate(y_cols, 1):\n",
    "            print(f\"\\n[{tag} | Var {c_idx}/{len(y_cols)}] {col}\")\n",
    "            yv = Y_sub[col].to_numpy(dtype=np.float32)\n",
    "\n",
    "            # Moran's I\n",
    "            t_m = time.time()\n",
    "            I, S0 = morans_I_stream(yv, idx, d2, inverse_distance=inverse_distance)\n",
    "            print(f\"[Moran] I={I:.6f} (took {time.time()-t_m:.2f} sec)\")\n",
    "\n",
    "            # CV R^2 (reuse splits)\n",
    "            print(\"[CV] Cross-validated Ridge R^2 ...\")\n",
    "            t_cv = time.time()\n",
    "            r2_mean, r2_sd = cv_r2_linear_from_X(X_std, yv, splits, alpha=alpha)\n",
    "            print(f\"[CV] mean={r2_mean:.4f}, sd={r2_sd:.4f} (took {time.time()-t_cv:.2f} sec)\")\n",
    "\n",
    "            # Store\n",
    "            results[tag][col] = {\n",
    "                \"morans_I\": float(I),\n",
    "                \"S0\": float(S0),\n",
    "                \"cv_r2_mean\": float(r2_mean),\n",
    "                \"cv_r2_sd\": float(r2_sd),\n",
    "                \"n_cells_subsample\": int(X_std.shape[0]),\n",
    "                \"k\": int(k),\n",
    "                \"weights\": \"inverse_distance\" if inverse_distance else \"binary\",\n",
    "                \"seed\": int(seed),\n",
    "                \"shared_cells\": True,\n",
    "                \"n_splits\": int(n_splits),\n",
    "                \"alpha\": float(alpha),\n",
    "            }\n",
    "            tidy_rows.append({\n",
    "                \"embedding\": tag,\n",
    "                \"variable\": col,\n",
    "                \"morans_I\": float(I),\n",
    "                \"S0\": float(S0),\n",
    "                \"cv_r2_mean\": float(r2_mean),\n",
    "                \"cv_r2_sd\": float(r2_sd),\n",
    "                \"n_cells_subsample\": int(X_std.shape[0]),\n",
    "                \"k\": int(k),\n",
    "                \"weights\": \"inverse_distance\" if inverse_distance else \"binary\",\n",
    "                \"seed\": int(seed),\n",
    "                \"shared_cells\": True,\n",
    "                \"n_splits\": int(n_splits),\n",
    "                \"alpha\": float(alpha),\n",
    "            })\n",
    "\n",
    "        print(f\"=== Done with embedding: {tag}. Total time {time.time()-t_total:.2f} sec ===\")\n",
    "\n",
    "    tidy_df = pd.DataFrame(tidy_rows).set_index([\"embedding\", \"variable\"]).sort_index()\n",
    "    return results, tidy_df, sub_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6cbe5-672c-40a2-80b7-390858ee2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, tidy_df, shared_cells = compare_embeddings_subsampled_samecells_multi(\n",
    "    embeddings_df=gcn_emb_df.set_index('cell_id').loc[:,['0','1','2','3','4','5','6','7','8','9',]],\n",
    "     embeddings_df_old=gcn_emb_nopathway_df.set_index('cell_id').loc[:,['0','1','2','3','4','5','6','7','8','9',]],\n",
    "    Y_df=all_pw,                 # <-- DataFrame of variables (index = cell names)\n",
    "    k=15,\n",
    "    subsample_n=300_000,\n",
    "    inverse_distance=True,\n",
    "    seed=42,\n",
    "    n_splits=5,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "print(tidy_df)                 # one row per (embedding, variable)\n",
    "# shared_cells is the index of the cells used everywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839e476-7ecc-4fd4-98f7-3f7a21e9615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_df.to_csv('../../results/figures_Fig1/results_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd382ee-d351-4a49-952d-f65cfa00ec2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fb40f-76e2-4d05-be4f-0fb59a10ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make wide comparison: variables as rows, embeddings as columns\n",
    "r2_wide = tidy_df[\"cv_r2_mean\"].unstack(level=\"embedding\")\n",
    "r2_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116ca68-6543-4fee-b846-dd141fd8ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_wide.to_csv('../../results/figures_Fig1/r2_pathway_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d45bf-9bfe-4152-9106-a3f30ee8375e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
